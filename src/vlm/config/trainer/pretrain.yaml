name: default
debug: true
default_root_dir: ${hydra:runtime.cwd}
experiment_name: vlm_training
max_epochs: 1
save_top_k: 3
monitor_metric: val_loss
monitor_mode: min
early_stopping: false
patience: 5
log_every_n_steps: 50
val_check_interval: 0.5
gradient_clip_val: 1.0
accumulate_grad_batches: 1
precision: 16-mixed
accelerator: gpu
devices: 1
strategy: null
resume_from_checkpoint: true
checkpoint_path: null
wandb_project_name: vlm-training
log_model_to_wandb: true
num_training_samples: 558000
batch_size: 16

defaults:
  - unfreeze: pretrain
  - learning_rate: default
  - weight_decay: default
