{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visual encoder -> 196 token 768\n",
    "llm -> tokenizer -> embed -> 10 token 4096\n",
    "connector = project (768 -> 4096) + concat\n",
    "project -> 196 token 4096\n",
    "206 token 4096\n",
    "196 token + 10 token\n",
    "position_ids\n",
    "llm(position_ids=, inputs_embed=, mask=[1, 1, 1, 1, 1, 1])\n",
    "decode\n",
    "\n",
    "<image>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `vlm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1131\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:833\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    834\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'vlm'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/pasteur/u/yiming/small-vlm/outputs/2025-05-12/01-42-28/checkpoint-3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1133\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1134\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1139\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1140\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1141\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1142\u001b[39m         )\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1145\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `vlm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"/pasteur/u/yiming/small-vlm/outputs/2025-05-12/01-42-28/checkpoint-3\", trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image = torch.load(\"/pasteur/u/yiming/small-vlm/outputs/2025-05-11/21-06-37/images.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4707,  0.4551,  0.4707,  ...,  0.4707,  0.5586,  0.5156],\n",
       "          [ 0.5586,  0.5742,  0.5430,  ..., -0.7695, -0.5820, -0.2734],\n",
       "          [ 0.6328,  0.6016,  0.5859,  ..., -1.5312, -1.5625, -1.5312],\n",
       "          ...,\n",
       "          [-0.0845, -0.1279, -0.1572,  ..., -1.3125, -1.2656, -1.2109],\n",
       "          [-0.1279, -0.1279, -0.1865,  ..., -1.3125, -1.2109, -1.1953],\n",
       "          [-0.1572, -0.0986, -0.2012,  ..., -1.3125, -1.2109, -1.2344]],\n",
       "\n",
       "         [[ 0.6484,  0.6328,  0.6484,  ...,  0.7539,  0.8438,  0.8008],\n",
       "          [ 0.7383,  0.7539,  0.7227,  ..., -0.5195, -0.3262, -0.0413],\n",
       "          [ 0.8125,  0.7852,  0.7695,  ..., -1.3281, -1.3594, -1.3438],\n",
       "          ...,\n",
       "          [ 0.0038, -0.0413, -0.0413,  ..., -1.1094, -1.0625, -0.9883],\n",
       "          [-0.0413, -0.0413, -0.0713,  ..., -1.1406, -1.0312, -1.0000],\n",
       "          [-0.0713, -0.0112, -0.0864,  ..., -1.1406, -1.0312, -1.0469]],\n",
       "\n",
       "         [[ 0.8789,  0.8672,  0.8516,  ...,  0.8672,  0.9531,  0.9102],\n",
       "          [ 0.9648,  0.9805,  0.9219,  ..., -0.3418, -0.1582,  0.1270],\n",
       "          [ 1.0078,  0.9805,  0.9648,  ..., -1.0938, -1.1250, -1.1094],\n",
       "          ...,\n",
       "          [ 0.1836,  0.1406,  0.1270,  ..., -0.8555, -0.8398, -0.8398],\n",
       "          [ 0.1406,  0.1406,  0.0981,  ..., -0.8555, -0.7695, -0.8125],\n",
       "          [ 0.1123,  0.1689,  0.0840,  ..., -0.8555, -0.7695, -0.8555]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 21:09:54,385] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14-336\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "output = model(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.1939, -0.4720, -0.2071,  ...,  0.2971,  0.5958, -0.2120],\n",
       "         [ 0.3681,  1.0474, -0.3662,  ...,  0.5626, -0.5293, -0.3300],\n",
       "         [ 1.8667,  0.9950, -0.3714,  ...,  2.3189,  0.1522,  0.9149],\n",
       "         ...,\n",
       "         [ 0.4588,  0.8863, -0.3187,  ...,  0.6389,  0.0093, -0.0024],\n",
       "         [ 0.4796,  0.7912, -0.6391,  ...,  0.4792, -0.0388,  0.1317],\n",
       "         [ 0.0817,  1.1452, -0.5089,  ...,  0.4315, -0.1363,  0.0750]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), pooler_output=tensor([[ 0.3666, -0.8041, -0.4335,  ...,  0.7717,  1.3503, -0.4265]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "\n",
    "\n",
    "def create_dynamic_vlm_class(\n",
    "    base_language_model_name_or_path: str,  # e.g., \"google/gemma-3-4b-it\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Dynamically creates a VLM class that inherits from the specified base LLM's\n",
    "    *ForCausalLM model class.\n",
    "    \"\"\"\n",
    "    # Determine the base LLM's actual config class and model class\n",
    "    base_language_model_actual_config = AutoConfig.from_pretrained(\n",
    "        base_language_model_name_or_path, trust_remote_code=True\n",
    "    )\n",
    "    base_language_model_config_class = base_language_model_actual_config.__class__\n",
    "    if base_language_model_config_class not in MODEL_FOR_CAUSAL_LM_MAPPING:\n",
    "        raise ValueError(\n",
    "            f\"Config class {base_language_model_config_class.__name__} for LLM {base_language_model_name_or_path} \"\n",
    "            f\"not in MODEL_FOR_CAUSAL_LM_MAPPING. Cannot determine parent CausalLM class.\"\n",
    "        )\n",
    "    ParentLLMClass = MODEL_FOR_CAUSAL_LM_MAPPING[base_language_model_config_class]\n",
    "    return ParentLLMClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 03:20:40,988] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "VLM = create_dynamic_vlm_class(\"lmsys/vicuna-7b-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_config = \"lmsys/vicuna-7b-v1.5\"\n",
    "config = AutoConfig.from_pretrained(text_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 06:30:49,248] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vlm.models.configuration_vlm import VLMConfig\n",
    "from transformers import CLIPVisionConfig, AutoConfig\n",
    "\n",
    "vision_config = {\n",
    "    \"name\": \"clip\",\n",
    "    \"hf_name\": \"openai/clip-vit-base-patch16\",\n",
    "    \"model_type\": \"huggingface\",\n",
    "}\n",
    "text_config = \"lmsys/vicuna-7b-v1.5\"\n",
    "config = AutoConfig.from_pretrained(text_config)\n",
    "print(config)\n",
    "projector_config = {\"name\": \"projector\"}\n",
    "\n",
    "\n",
    "configuration = VLMConfig(config.to_dict(), vision_config, projector_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionConfig {\n",
      "  \"hf_name\": \"openai/clip-vit-base-patch16\",\n",
      "  \"model_type\": \"vision_model\",\n",
      "  \"name\": \"clip\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(configuration.vision_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "a = OmegaConf.create(configuration.vision_config.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', '_attn_implementation_autoset': False, 'transformers_version': '4.51.3', 'name': 'clip', 'hf_name': 'openai/clip-vit-base-patch16', 'model_type': 'vision_model'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm.models.modeling_vlm import create_dynamic_vlm_class\n",
    "\n",
    "VLM = create_dynamic_vlm_class(\"lmsys/vicuna-7b-v1.5\", \"VLM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 18600.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModelConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"language_model\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "VisionConfig {\n",
      "  \"hf_name\": \"openai/clip-vit-base-patch16\",\n",
      "  \"model_type\": \"vision_model\",\n",
      "  \"name\": \"clip\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n",
      "ProjectorConfig {\n",
      "  \"model_type\": \"projector\",\n",
      "  \"name\": \"projector\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = VLM.from_pretrained(\n",
    "    \"lmsys/vicuna-7b-v1.5\", config=configuration, torch_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs_embeds = torch.load(\"inputs_embeds.pt\").to(\"cuda\")\n",
    "attention_mask = torch.load(\"attention_mask.pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 623, 4096])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-11 03:29:11,636] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.38s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "original_model = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        \"lmsys/vicuna-7b-v1.5\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = original_model.generate(attention_mask=attention_mask, inputs_embeds=inputs_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = original_model(attention_mask=attention_mask, inputs_embeds=inputs_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  450,  1967,  5692,   304,   367,   263,  3802, 29899,   786, 10322,\n",
       "           310,   263,  2022, 29915, 29879,  3700, 29892, 10075,  4586,   411,\n",
       "           263,  3802, 29899,   786,   301,   575,   470,   263, 11758,   301,\n",
       "           575, 29889,   450,  2022, 29915, 29879,  5076,   526,  1722,   322,\n",
       "          1009,  2258,  1455,  5680,   526,  7962, 29892,  3704,  1009,   321,\n",
       "         29891,   295,  1161,   267,   322,   321, 29891,   774,  5727, 29889,\n",
       "           450,  3239,   310,   278,  1967,   338,  1999,   332,   719,   322,\n",
       "           714,   310,  8569, 29892,   607, 14661,   393,   278,  1967,   471,\n",
       "          4586,   411,   263,  1407,  4091,   340, 10809,   310,  1746, 29889,\n",
       "           450,  2022, 29915, 29879, 19309, 16225,  5692,   304,   367, 10029,\n",
       "         14294,   322,   756,   263,  7248,   330,   677, 29892,   607,  1033,\n",
       "           367,  2861,   304,   278,   671,   310,   263, 11013,   470,   263,\n",
       "          4964,  3578,  2752, 29889,  6811,   497, 29892,   278,  1967,   756,\n",
       "           263,  1407,  1880,  3233,   310,  9493,   322,  7542,   537, 29892,\n",
       "           322,   278,  2022, 29915, 29879,  3700,   338,   278,  1667,  8569,\n",
       "           310,   278,  1967, 29889,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-12 06:02:34,215] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[    1,   319, 13563,  1546,   263, 12758,  5199,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  5199, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  4002, 29581,\n",
      "           445,  1967, 29889,   319,  1799,  9047, 13566, 29901]],\n",
      "       device='cuda:0')\n",
      "images_tensor tensor([[[[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          ...,\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
      "\n",
      "         [[-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          ...,\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112],\n",
      "          [-0.0112, -0.0112, -0.0112,  ..., -0.0112, -0.0112, -0.0112]],\n",
      "\n",
      "         [[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          ...,\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "          [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "image_sizes [(681, 336)]\n",
      "forward_output CausalLMOutputWithPast(loss=None, logits=tensor([[[  0.8828,   0.3223,  -0.7305,  ...,   1.7031,   2.7188,   1.2344],\n",
      "         [-10.1875,  -7.3125,  -1.2578,  ...,  -7.5625,  -9.1250,  -8.9375],\n",
      "         [ -7.5000,  -7.5312,   8.1250,  ...,  -6.1250,  -4.6562,  -5.6875],\n",
      "         ...,\n",
      "         [ -7.9688,  -2.8125,   6.3125,  ...,  -4.7812,  -8.0625,  -6.6562],\n",
      "         [ -7.9688,  -4.2500,   7.4062,  ...,  -6.6875,  -7.4688,  -6.3125],\n",
      "         [ -7.5312,  -4.3438,   5.0312,  ...,  -5.0312,  -5.5625,  -4.8750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_ids tensor([[  263, 29871, 29896, 29900, 29889, 29945, 29889,   263, 29871, 29896,\n",
      "         29900, 29889, 29945, 29889,   263, 29871, 29896, 29900, 29889, 29945,\n",
      "         29889,   263, 29871, 29896, 29900, 29889, 29945, 29889,   263, 29871,\n",
      "         29896, 29900, 29889, 29945, 29889,   263, 29871, 29896, 29900, 29889,\n",
      "         29945, 29889,   263, 29871, 29896, 29900, 29889, 29945, 29889,   263,\n",
      "         29871, 29896, 29900, 29889, 29945, 29889,   263, 29871, 29896, 29900,\n",
      "         29889, 29945, 29889,   263, 29871, 29896, 29900, 29889, 29945, 29889,\n",
      "           263, 29871, 29896, 29900, 29889, 29945, 29889,   263, 29871, 29896,\n",
      "         29900, 29889, 29945, 29889,   263, 29871, 29896, 29900, 29889, 29945,\n",
      "         29889,   263, 29871, 29896, 29900, 29889, 29945, 29889,   263, 29871,\n",
      "         29896, 29900, 29889, 29945, 29889,   263, 29871, 29896, 29900, 29889,\n",
      "         29945, 29889,   263, 29871, 29896, 29900, 29889, 29945, 29889,   263,\n",
      "         29871, 29896, 29900, 29889, 29945, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29900, 29889, 29945, 29889,   263, 29871, 29896,\n",
      "         29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29900, 29889,\n",
      "         29945, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889,   263, 29871, 29896, 29889,   263, 29871, 29896, 29889,\n",
      "           263, 29871, 29896, 29889,   263, 29871, 29896, 29889,   263, 29871,\n",
      "         29896, 29889]], device='cuda:0')\n",
      "a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 10.5. a 1. a 10.5. a 1. a 1. a 10.5. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1. a 1.\n"
     ]
    }
   ],
   "source": [
    "from vlm.inference.eval import eval_model\n",
    "\n",
    "eval_model(\n",
    "    \"/pasteur/u/yiming/small-vlm/outputs/2025-05-12/05-41-15/checkpoint-5\",\n",
    "    \"Describe this image.\",\n",
    "    \"/pasteur/u/yiming/small-vlm/example/image.jpg\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._language_model._language_model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0027, -0.0041,  0.0021,  ..., -0.0098,  0.0035, -0.0041],\n",
       "         [-0.0098, -0.0132, -0.0128,  ...,  0.0089,  0.0129, -0.0085],\n",
       "         [ 0.0217, -0.0081,  0.0075,  ...,  0.0138, -0.0018, -0.0225],\n",
       "         ...,\n",
       "         [-0.0178, -0.0002,  0.0173,  ...,  0.0247,  0.0054,  0.0096],\n",
       "         [ 0.0063, -0.0166,  0.0113,  ..., -0.0105,  0.0145,  0.0071],\n",
       "         [ 0.0032,  0.0017,  0.0056,  ..., -0.0054,  0.0162,  0.0022]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model._language_model._language_model.generate(\n",
    "    attention_mask=attention_mask, inputs_embeds=inputs_embeds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model._language_model._language_model(\n",
    "    attention_mask=attention_mask, inputs_embeds=inputs_embeds\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[  0.8750,   0.2539,  -0.7070,  ...,   1.7031,   2.7031,   1.2266],\n",
       "         [-10.1250,  -7.2812,  -1.1875,  ...,  -7.5625,  -9.0000,  -8.9375],\n",
       "         [ -7.5312,  -7.7500,   8.0000,  ...,  -6.1562,  -4.6562,  -5.7188],\n",
       "         ...,\n",
       "         [-12.0000,  -4.4688,   8.0000,  ...,  -5.8750,  -8.7500,  -8.1875],\n",
       "         [-11.8125,  -4.2500,   6.4375,  ...,  -7.0625,  -7.4688,  -9.3125],\n",
       "         [ -9.4375,  -4.6875,   9.5625,  ...,  -4.8125,  -5.0312,  -5.8125]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7f84105ad090>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.1280, -0.1134, -0.1572,  ..., -0.1426,  0.7187,  1.3318],\n",
       "          [-0.1718, -0.1280, -0.1572,  ...,  0.3099,  0.9960,  1.2296],\n",
       "          [-0.1572, -0.0988, -0.1134,  ...,  0.7479,  1.2150,  1.0836],\n",
       "          ...,\n",
       "          [ 0.0033, -0.0550, -0.0696,  ...,  0.0325,  0.1493,  0.2223],\n",
       "          [ 0.0179, -0.0550, -0.0696,  ...,  0.1201,  0.2515,  0.3099],\n",
       "          [ 0.0325, -0.0113, -0.0259,  ...,  0.1785,  0.2223,  0.1639]],\n",
       " \n",
       "         [[-0.0562, -0.0412, -0.0862,  ..., -0.1913,  0.6942,  1.3845],\n",
       "          [-0.1012, -0.0562, -0.0862,  ...,  0.2589,  0.9793,  1.2795],\n",
       "          [-0.0862, -0.0262, -0.0412,  ...,  0.7092,  1.2044,  1.1144],\n",
       "          ...,\n",
       "          [ 0.0338, -0.0262, -0.0412,  ..., -0.0112,  0.0789,  0.1539],\n",
       "          [ 0.0488, -0.0262, -0.0412,  ...,  0.0789,  0.1839,  0.2439],\n",
       "          [ 0.0638,  0.0188,  0.0038,  ...,  0.1389,  0.1539,  0.0939]],\n",
       " \n",
       "         [[-0.3000, -0.2857, -0.3284,  ..., -0.2146,  0.6244,  1.2216],\n",
       "          [-0.3426, -0.3000, -0.3284,  ...,  0.2546,  0.8945,  1.1221],\n",
       "          [-0.3284, -0.2715, -0.2857,  ...,  0.6812,  1.1078,  1.0083],\n",
       "          ...,\n",
       "          [-0.3284, -0.3853, -0.3995,  ..., -0.2715, -0.1720, -0.1009],\n",
       "          [-0.3142, -0.3853, -0.3995,  ..., -0.1862, -0.0724, -0.0156],\n",
       "          [-0.3000, -0.3426, -0.3568,  ..., -0.1293, -0.1009, -0.1578]]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/pasteur/u/yiming/small-vlm/example/image.jpg\")\n",
    "image = image.convert(\"RGB\")\n",
    "model.visual_encoder.preprocessor(image)[\"pixel_values\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "input_ids = Tensor(\n",
    "    [\n",
    "        [\n",
    "            1,\n",
    "            319,\n",
    "            13563,\n",
    "            1546,\n",
    "            263,\n",
    "            12758,\n",
    "            1404,\n",
    "            322,\n",
    "            385,\n",
    "            23116,\n",
    "            21082,\n",
    "            20255,\n",
    "            29889,\n",
    "            450,\n",
    "            20255,\n",
    "            4076,\n",
    "            8444,\n",
    "            29892,\n",
    "            13173,\n",
    "            29892,\n",
    "            322,\n",
    "            1248,\n",
    "            568,\n",
    "            6089,\n",
    "            304,\n",
    "            278,\n",
    "            1404,\n",
    "            29915,\n",
    "            29879,\n",
    "            5155,\n",
    "            29889,\n",
    "            3148,\n",
    "            1001,\n",
    "            29901,\n",
    "            29871,\n",
    "            -200,\n",
    "            29871,\n",
    "            13,\n",
    "            3624,\n",
    "            278,\n",
    "            1081,\n",
    "            2527,\n",
    "            304,\n",
    "            278,\n",
    "            2175,\n",
    "            470,\n",
    "            304,\n",
    "            278,\n",
    "            1492,\n",
    "            310,\n",
    "            278,\n",
    "            2022,\n",
    "            393,\n",
    "            278,\n",
    "            10992,\n",
    "            23090,\n",
    "            338,\n",
    "            304,\n",
    "            278,\n",
    "            1492,\n",
    "            310,\n",
    "            29973,\n",
    "            13,\n",
    "            22550,\n",
    "            278,\n",
    "            1139,\n",
    "            773,\n",
    "            263,\n",
    "            2323,\n",
    "            1734,\n",
    "            470,\n",
    "            16549,\n",
    "            29889,\n",
    "            319,\n",
    "            1799,\n",
    "            9047,\n",
    "            13566,\n",
    "            29901,\n",
    "        ]\n",
    "    ],\n",
    ")\n",
    "attention_mask = Tensor([\n",
    "    [\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "        True,\n",
    "    ]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1.]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0027, -0.0041,  0.0021,  ..., -0.0098,  0.0035, -0.0041],\n",
      "         [-0.0098, -0.0132, -0.0128,  ...,  0.0089,  0.0129, -0.0085],\n",
      "         [ 0.0217, -0.0081,  0.0075,  ...,  0.0138, -0.0018, -0.0225],\n",
      "         ...,\n",
      "         [-0.0178, -0.0002,  0.0173,  ...,  0.0247,  0.0054,  0.0096],\n",
      "         [ 0.0063, -0.0166,  0.0113,  ..., -0.0105,  0.0145,  0.0071],\n",
      "         [ 0.0032,  0.0017,  0.0056,  ..., -0.0054,  0.0162,  0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<StackBackward0>)\n",
      "{'pad_token_id': 0, 'num_beams': 1, 'use_cache': True}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建符合要求尺寸的零张量 [batch_size, channels, height, width]\n",
    "# CLIP模型通常使用3通道RGB图像\n",
    "dummy_image = torch.zeros(1, 3, 336, 336).to(\"cuda\").to(torch.bfloat16)\n",
    "input_ids = input_ids.to(\"cuda\").to(torch.int32)\n",
    "attention_mask = attention_mask.to(\"cuda\").to(torch.bfloat16)\n",
    "model = model.to(\"cuda\")\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    images=dummy_image,\n",
    "    pad_token_id=0,\n",
    "    # do_sample=False,\n",
    "    # temperature=0.0,\n",
    "    # top_p=1.0,\n",
    "    num_beams=1,\n",
    "    # max_new_tokens=16,\n",
    "    use_cache=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13, 29909,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3831\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3831\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3838\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3870\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3867\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3868\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3870\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3875\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/yiming/small-vlm/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:668\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    667\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    671\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    674\u001b[39m )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mTypeError\u001b[39m: argument 'ids': 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
